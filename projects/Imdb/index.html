<!DOCTYPE html> <html lang="en"> <head> <meta http-equiv="Content-Type" content="text/html; charset=UTF-8"> <meta charset="utf-8"> <meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no"> <meta http-equiv="X-UA-Compatible" content="IE=edge"> <title> IMDB Classificatoin using BERT | Sanjan B M </title> <meta name="author" content="Sanjan B M"> <meta name="description" content="Developed a text classification model leveraging BERT (Bidirectional Encoder Representations from Transformers)."> <meta name="keywords" content="jekyll, jekyll-theme, academic-website, portfolio-website"> <link rel="stylesheet" href="/assets/css/bootstrap.min.css?a4b3f509e79c54a512b890d73235ef04"> <link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/mdbootstrap@4.20.0/css/mdb.min.css" integrity="sha256-jpjYvU3G3N6nrrBwXJoVEYI/0zw8htfFnhT9ljN3JJw=" crossorigin="anonymous"> <link defer rel="stylesheet" href="/assets/css/academicons.min.css?f0b7046b84e425c55f3463ac249818f5"> <link defer rel="stylesheet" href="/assets/css/scholar-icons.css?62b2ac103a88034e6882a5be5f3e2772"> <link defer rel="stylesheet" type="text/css" href="https://fonts.googleapis.com/css?family=Roboto:300,400,500,700|Roboto+Slab:100,300,400,500,700|Material+Icons&amp;display=swap"> <link defer rel="stylesheet" href="/assets/css/jekyll-pygments-themes-github.css?591dab5a4e56573bf4ef7fd332894c99" media="" id="highlight_theme_light"> <link rel="shortcut icon" href="data:image/svg+xml,&lt;svg%20xmlns=%22http://www.w3.org/2000/svg%22%20viewBox=%220%200%20100%20100%22&gt;&lt;text%20y=%22.9em%22%20font-size=%2290%22&gt;%E2%9A%9B&lt;/text&gt;&lt;/svg&gt;"> <link rel="stylesheet" href="/assets/css/main.css?d41d8cd98f00b204e9800998ecf8427e"> <link rel="canonical" href="https://sanjanb.github.io/projects/Imdb/"> <script src="/assets/js/theme.js?a81d82887dd692e91686b43de4542f18"></script> <link defer rel="stylesheet" href="/assets/css/jekyll-pygments-themes-native.css?5847e5ed4a4568527aa6cfab446049ca" media="none" id="highlight_theme_dark"> <script>
    initTheme();
  </script> </head> <body class="fixed-top-nav "> <header> <nav id="navbar" class="navbar navbar-light navbar-expand-sm fixed-top" role="navigation"> <div class="container"> <a class="navbar-brand title font-weight-lighter" href="/"> Sanjan B M </a> <button class="navbar-toggler collapsed ml-auto" type="button" data-toggle="collapse" data-target="#navbarNav" aria-controls="navbarNav" aria-expanded="false" aria-label="Toggle navigation"> <span class="sr-only">Toggle navigation</span> <span class="icon-bar top-bar"></span> <span class="icon-bar middle-bar"></span> <span class="icon-bar bottom-bar"></span> </button> <div class="collapse navbar-collapse text-right" id="navbarNav"> <ul class="navbar-nav ml-auto flex-nowrap"> <li class="nav-item "> <a class="nav-link" href="/">About </a> </li> <li class="nav-item "> <a class="nav-link" href="/blog/">Blogs </a> </li> <li class="nav-item active"> <a class="nav-link" href="/projects/">Projects <span class="sr-only">(current)</span> </a> </li> <li class="nav-item "> <a class="nav-link" href="/cv/">CV </a> </li> <li class="nav-item "> <a class="nav-link" href="/repositories/">Repositories </a> </li> <li class="nav-item dropdown "> <a class="nav-link dropdown-toggle" href="#" id="navbarDropdown" role="button" data-toggle="dropdown" aria-haspopup="true" aria-expanded="false">submenus </a> <div class="dropdown-menu dropdown-menu-right" aria-labelledby="navbarDropdown"> <a class="dropdown-item " href="/books/">bookshelf</a> <div class="dropdown-divider"></div> <a class="dropdown-item " href="/repositories/">Repositories</a> <div class="dropdown-divider"></div> <a class="dropdown-item " href="/teaching/">Teaching</a> <div class="dropdown-divider"></div> <a class="dropdown-item " href="/people/">People</a> </div> </li> <li class="nav-item"> <button id="search-toggle" title="Search" onclick="openSearchModal()"> <span class="nav-link">ctrl k <i class="ti ti-search"></i></span> </button> </li> <li class="toggle-container"> <button id="light-toggle" title="Change theme"> <i class="ti ti-sun-moon" id="light-toggle-system"></i> <i class="ti ti-moon-filled" id="light-toggle-dark"></i> <i class="ti ti-sun-filled" id="light-toggle-light"></i> </button> </li> </ul> </div> </div> </nav> <progress id="progress" value="0"> <div class="progress-container"> <span class="progress-bar"></span> </div> </progress> </header> <div class="container mt-5" role="main"> <div class="post"> <header class="post-header"> <h1 class="post-title">IMDB Classificatoin using BERT</h1> <p class="post-description">Developed a text classification model leveraging BERT (Bidirectional Encoder Representations from Transformers).</p> </header> <article> <h1 id="bert-an-overview-of-architecture-training-and-bias">BERT: An Overview of Architecture, Training, and Bias</h1> <h2 id="abstract">Abstract</h2> <p>This paper explores the architecture, training process, and biases associated with BERT (Bidirectional Encoder Representations from Transformers), a transformative model in natural language processing (NLP). We delve into the components of BERT, its training methodologies, and the challenges posed by biases in its predictions.</p> <h2 id="table-of-contents">Table of Contents</h2> <ol> <li><a href="#introduction">Introduction</a></li> <li> <a href="#bert-architecture">BERT Architecture</a> <ul> <li><a href="#components">Components</a></li> <li><a href="#tokenization">Tokenization</a></li> </ul> </li> <li> <a href="#training-bert">Training BERT</a> <ul> <li><a href="#training-datasets">Training Datasets</a></li> <li><a href="#training-tasks">Training Tasks</a></li> <li><a href="#compute-resources">Compute Resources</a></li> </ul> </li> <li> <a href="#transfer-learning-with-bert">Transfer Learning with BERT</a> <ul> <li><a href="#components-1">Components</a></li> <li><a href="#resources">Resources</a></li> <li><a href="#benefits">Benefits</a></li> </ul> </li> <li> <a href="#biases-in-bert">Biases in BERT</a> <ul> <li><a href="#gender-bias-in-predictions">Gender Bias in Predictions</a></li> <li><a href="#implications-of-bias">Implications of Bias</a></li> <li><a href="#mitigation-strategies">Mitigation Strategies</a></li> </ul> </li> <li> <a href="#fine-tuning-bert-for-text-classification">Fine-Tuning BERT for Text Classification</a> <ul> <li><a href="#imdb-dataset">IMDB Dataset</a></li> <li><a href="#adding-a-linear-classifier">Adding a Linear Classifier</a></li> <li><a href="#dropout-layer">Dropout Layer</a></li> <li><a href="#training-process">Training Process</a></li> </ul> </li> <li> <a href="#experimental-setup">Experimental Setup</a> <ul> <li><a href="#hugging-face-libraries">Hugging Face Libraries</a></li> <li><a href="#data-preparation">Data Preparation</a></li> <li><a href="#visualizing-data">Visualizing Data</a></li> </ul> </li> <li> <a href="#results">Results</a> <ul> <li><a href="#training-and-evaluation">Training and Evaluation</a></li> <li><a href="#inference">Inference</a></li> </ul> </li> <li><a href="#conclusion">Conclusion</a></li> <li><a href="#references">References</a></li> </ol> <h2 id="introduction">Introduction</h2> <p>BERT has revolutionized the field of NLP by providing a robust framework for understanding and generating human language. This paper aims to provide a comprehensive overview of BERT’s architecture, training process, and the challenges posed by biases in its predictions.</p> <h2 id="bert-architecture">BERT Architecture</h2> <h3 id="components">Components</h3> <p>BERT’s architecture consists of two main components: an encoder and a decoder. The encoder processes input data, while the decoder generates output. Encoder-decoder models are suitable for generative tasks like translation and summarization. Encoder-only models, like BERT, excel in tasks requiring understanding of the input, such as text classification and named entity recognition. Decoder-only models, like GPT, are used for text generation.</p> <h3 id="tokenization">Tokenization</h3> <p>BERT uses subword tokenization, breaking words into smaller subwords or tokens. For example, “tokenization” is split into “token” and “##ization”. This approach ensures frequently used words are not split into smaller subwords and decomposes rarely used words into meaningful subwords. BERT uncased has a vocabulary of around 30,000 tokens, mapping each subword to a unique numerical ID.</p> <table> <thead> <tr> <th><strong>Model</strong></th> <th><strong>Vocabulary Size</strong></th> <th><strong>Tokenization Method</strong></th> </tr> </thead> <tbody> <tr> <td>BERT</td> <td>30,000</td> <td>WordPiece</td> </tr> <tr> <td>GPT-2</td> <td>50,000</td> <td>Byte-Pair Encoding</td> </tr> <tr> <td>GPT-3</td> <td>50,000</td> <td>Byte-Pair Encoding</td> </tr> </tbody> </table> <h2 id="training-bert">Training BERT</h2> <h3 id="training-datasets">Training Datasets</h3> <p>BERT was trained on the English Wikipedia (around 2.5 billion words) and BookCorpus (11,000 books with around 800 million words). The training process did not require labeled data, allowing BERT to be trained on raw text.</p> <h3 id="training-tasks">Training Tasks</h3> <p>BERT was trained using two key tasks: masked language modeling (predicting masked-out words in a sentence) and next sentence prediction (determining if one sentence logically follows another).</p> <h3 id="compute-resources">Compute Resources</h3> <p>Training BERT required significant compute resources, costing hundreds of thousands of dollars on Tensor Processing Units (TPUs).</p> <h2 id="transfer-learning-with-bert">Transfer Learning with BERT</h2> <h3 id="components-1">Components</h3> <p>Transfer learning with BERT involves two main components: pre-training and fine-tuning. Pre-training involves training a model on a large dataset, while fine-tuning adapts the pre-trained model to a specific task using labeled data.</p> <h3 id="resources">Resources</h3> <p>Pre-training is resource-intensive, requiring large datasets and significant computational power. Fine-tuning, however, is less resource-demanding and faster.</p> <h3 id="benefits">Benefits</h3> <p>Transfer learning allows for better accuracy and efficiency. Starting with a pre-trained model generally yields better results than training a model from scratch for specific tasks like sentiment analysis.</p> <h2 id="biases-in-bert">Biases in BERT</h2> <h3 id="gender-bias-in-predictions">Gender Bias in Predictions</h3> <p>BERT shows a strong gender bias in its predictions, associating certain professions with specific genders (e.g., nurses with females and doctors with males).</p> <h3 id="implications-of-bias">Implications of Bias</h3> <p>This bias can have significant implications, such as in job resume filtering, where AI systems might prefer resumes based on gender associations rather than qualifications.</p> <h3 id="mitigation-strategies">Mitigation Strategies</h3> <p>To mitigate these biases, it’s crucial to have human oversight in AI systems to ensure fair and unbiased outcomes.</p> <h2 id="fine-tuning-bert-for-text-classification">Fine-Tuning BERT for Text Classification</h2> <h3 id="imdb-dataset">IMDB Dataset</h3> <p>The IMDB dataset is used for training, where movie reviews are classified as positive (1) or negative (0).</p> <h3 id="adding-a-linear-classifier">Adding a Linear Classifier</h3> <p>To fine-tune BERT for text classification, a linear classifier layer is added on top of the final embedding of the [CLS] token. The [CLS] token is a special token added at the beginning of each input sequence and is used for classification tasks.</p> <h3 id="dropout-layer">Dropout Layer</h3> <p>A dropout layer is often added before the linear classifier to reduce overfitting. Dropout randomly sets a fraction of input units to zero during training, which helps the model generalize better.</p> <h3 id="training-process">Training Process</h3> <p>The model is fine-tuned using the labeled IMDB dataset. The text reviews are tokenized and fed into BERT, and the final embedding of the [CLS] token is passed through the linear classifier to predict the label (positive or negative). The model is trained to minimize the classification error by adjusting the weights of both the pre-trained BERT model and the newly added linear classifier. —</p> <h1 id="example-code-for-fine-tuning-bert">Example code for fine-tuning BERT</h1> <p>from transformers import AutoTokenizer, AutoModelForSequenceClassification, Trainer, TrainingArguments</p> <p>checkpoint = “distilbert-base-cased” tokenizer = AutoTokenizer.from_pretrained(checkpoint) model = AutoModelForSequenceClassification.from_pretrained(checkpoint, num_labels=2)</p> <p>def tokenize_function(batch): return tokenizer(batch[“text”], padding=True, truncation=True)</p> <h1 id="assuming-imdb_encoded-is-the-encoded-dataset">Assuming <code class="language-plaintext highlighter-rouge">imdb_encoded</code> is the encoded dataset</h1> <p>training_args = TrainingArguments( output_dir=”./results”, evaluation_strategy=”epoch”, learning_rate=2e-5, per_device_train_batch_size=16, per_device_eval_batch_size=16, num_train_epochs=2, weight_decay=0.01, )</p> <p>trainer = Trainer( model=model, args=training_args, train_dataset=imdb_encoded[“train”], eval_dataset=imdb_encoded[“validation”], tokenizer=tokenizer, )</p> <p>trainer.train()</p> <h2 id="experimental-setup">Experimental Setup</h2> <h3 id="hugging-face-libraries">Hugging Face Libraries</h3> <p>The Hugging Face libraries, including <code class="language-plaintext highlighter-rouge">transformers</code> and <code class="language-plaintext highlighter-rouge">datasets</code>, provide pre-trained models and tools for NLP tasks. These libraries allow for easy access to a wide range of datasets and facilitate the conversion of datasets to Pandas DataFrames for visualization and manipulation.</p> <h3 id="data-preparation">Data Preparation</h3> <p>The IMDB dataset is prepared by removing HTML tags using regular expressions and balancing the dataset to ensure an approximately equal number of positive and negative reviews.</p> <h3 id="visualizing-data">Visualizing Data</h3> <p>A box plot is used to visualize the distribution of review lengths, providing insights into the range and average length of reviews in the dataset.</p> <table> <thead> <tr> <th><strong>Review Length</strong></th> <th><strong>Count</strong></th> </tr> </thead> <tbody> <tr> <td>Short (&lt; 50)</td> <td>120</td> </tr> <tr> <td>Medium (50-150)</td> <td>350</td> </tr> <tr> <td>Long (&gt; 150)</td> <td>50</td> </tr> </tbody> </table> <h2 id="results">Results</h2> <h3 id="training-and-evaluation">Training and Evaluation</h3> <p>The fine-tuned BERT model achieved an accuracy of approximately 89.5% on the validation set after two epochs of training. The model demonstrated effective learning and generalization on the IMDB dataset.</p> <h3 id="inference">Inference</h3> <p>The fine-tuned model can classify new movie reviews as positive or negative based on the learned patterns and context from the training data.</p> <h2 id="conclusion">Conclusion</h2> <p>BERT’s architecture and training methodologies have significantly advanced the field of NLP. However, addressing biases in its predictions remains a critical challenge. Future research should focus on developing more equitable and unbiased AI systems.</p> <h2 id="references">References</h2> <ul> <li>Devlin, J., Chang, M.-W., Lee, K., &amp; Toutanova, K. (2018). BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding. arXiv preprint arXiv:1810.04805.</li> <li> <p>Liu, Y., Ott, M., Goyal, N., Du, J., Joshi, M., Chen, D., … &amp; Stoyanov, V. (2019). RoBERTa: A Robustly Optimized BERT Pretraining Approach. arXiv preprint arXiv:1907.11692.</p> <hr> <p>layout: page title: project description: a project with a background image img: /assets/img/12.jpg —</p> </li> </ul> </article> </div> </div> <footer class="fixed-bottom" role="contentinfo"> <div class="container mt-0"> © Copyright 2025 Sanjan B M. Powered by <a href="https://jekyllrb.com/" target="_blank" rel="external nofollow noopener">Jekyll</a> with <a href="https://github.com/alshedivat/al-folio" rel="external nofollow noopener" target="_blank">al-folio</a> theme. Hosted by <a href="https://pages.github.com/" target="_blank" rel="external nofollow noopener">GitHub Pages</a>. Photos from <a href="https://unsplash.com" target="_blank" rel="external nofollow noopener">Unsplash</a>. </div> </footer> <script src="https://cdn.jsdelivr.net/npm/jquery@3.6.0/dist/jquery.min.js" integrity="sha256-/xUj+3OJU5yExlq6GSYGSHk7tPXikynS7ogEvDej/m4=" crossorigin="anonymous"></script> <script src="/assets/js/bootstrap.bundle.min.js"></script> <script src="https://cdn.jsdelivr.net/npm/mdbootstrap@4.20.0/js/mdb.min.js" integrity="sha256-NdbiivsvWt7VYCt6hYNT3h/th9vSTL4EDWeGs5SN3DA=" crossorigin="anonymous"></script> <script defer src="https://cdn.jsdelivr.net/npm/masonry-layout@4.2.2/dist/masonry.pkgd.min.js" integrity="sha256-Nn1q/fx0H7SNLZMQ5Hw5JLaTRZp0yILA/FRexe19VdI=" crossorigin="anonymous"></script> <script defer src="https://cdn.jsdelivr.net/npm/imagesloaded@5.0.0/imagesloaded.pkgd.min.js" integrity="sha256-htrLFfZJ6v5udOG+3kNLINIKh2gvoKqwEhHYfTTMICc=" crossorigin="anonymous"></script> <script defer src="/assets/js/masonry.js?a0db7e5d5c70cc3252b3138b0c91dcaf" type="text/javascript"></script> <script defer src="https://cdn.jsdelivr.net/npm/medium-zoom@1.1.0/dist/medium-zoom.min.js" integrity="sha256-ZgMyDAIYDYGxbcpJcfUnYwNevG/xi9OHKaR/8GK+jWc=" crossorigin="anonymous"></script> <script defer src="/assets/js/zoom.js?85ddb88934d28b74e78031fd54cf8308"></script> <script src="/assets/js/no_defer.js?2781658a0a2b13ed609542042a859126"></script> <script defer src="/assets/js/common.js?e0514a05c5c95ac1a93a8dfd5249b92e"></script> <script defer src="/assets/js/copy_code.js?c8a01c11a92744d44b093fc3bda915df" type="text/javascript"></script> <script defer src="/assets/js/jupyter_new_tab.js?d9f17b6adc2311cbabd747f4538bb15f"></script> <script async src="https://d1bxh8uas1mnw7.cloudfront.net/assets/embed.js"></script> <script async src="https://badge.dimensions.ai/badge.js"></script> <script defer type="text/javascript" id="MathJax-script" src="https://cdn.jsdelivr.net/npm/mathjax@3.2.2/es5/tex-mml-chtml.js" integrity="sha256-MASABpB4tYktI2Oitl4t+78w/lyA+D7b/s9GEP0JOGI=" crossorigin="anonymous"></script> <script src="/assets/js/mathjax-setup.js?a5bb4e6a542c546dd929b24b8b236dfd"></script> <script defer src="https://cdnjs.cloudflare.com/polyfill/v3/polyfill.min.js?features=es6" crossorigin="anonymous"></script> <script defer src="/assets/js/progress-bar.js?2f30e0e6801ea8f5036fa66e1ab0a71a" type="text/javascript"></script> <script src="/assets/js/vanilla-back-to-top.min.js?f40d453793ff4f64e238e420181a1d17"></script> <script>
    addBackToTop();
  </script> <script type="module" src="/assets/js/search/ninja-keys.min.js?a3446f084dcaecc5f75aa1757d087dcf"></script> <ninja-keys hidebreadcrumbs noautoloadmdicons placeholder="Type to start searching"></ninja-keys> <script src="/assets/js/search-setup.js?6c304f7b1992d4b60f7a07956e52f04a"></script> <script src="/assets/js/search-data.js"></script> <script src="/assets/js/shortcut-key.js?6f508d74becd347268a7f822bca7309d"></script> </body> </html>